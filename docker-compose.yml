version: "3.8"

x-common-env-vars:
  &common-env-vars
  DOCKERIZED: "True"
  REDIS_HOST: "redis"
  POSTGRES_HOST: "postgres"

services:
  webserver:
    image: "ghcr.io/modularhistory/webserver:$SHA"
    depends_on:
      - next
    deploy:
      restart_policy:
        condition: on-failure
    env_file: .env
    environment:
      DOMAINS: "modularhistory.com,www.modularhistory.com"
    ports:
      - "80:8080"
      - "443:8443"
    volumes:
      - letsencrypt:/etc/letsencrypt
      - certbot:/var/www/certbot
      - ./_volumes/media:/modularhistory/_volumes/media
      - ./_volumes/static:/modularhistory/_volumes/static
      - ./_volumes/redirects:/modularhistory/_volumes/redirects
      # Keep the nginx config in a volume to allow for zero-downtime deploys via
      # `docker compose exec webserver /usr/sbin/nginx -s reload`. Without this, the
      # container would always have to be stopped (so its static IP address becomes 
      # available) before starting a replacement container with an updated config.
      - ./config/nginx/prod:/etc/nginx/conf.d
      - ./config/nginx:/modularhistory/config/nginx

  certbot:
    # https://hub.docker.com/r/certbot/certbot/
    image: "certbot/certbot"
    entrypoint: >
      /bin/sh -c 'trap exit TERM; while :; do certbot renew; sleep 12h &
      wait $${!}; done;'
    volumes:
      - letsencrypt:/etc/letsencrypt
      - certbot:/var/www/certbot

  next:
    image: "ghcr.io/modularhistory/next:$SHA"
    command: npm run start
    depends_on:
      - django
    deploy:
      restart_policy:
        condition: on-failure
    env_file: .env
    environment:
      NEXTAUTH_URL: "https://www.modularhistory.com"
      NEXTAUTH_URL_INTERNAL: "http://next:3000"
    expose:
      - "3000"
    volumes:
      - ./_volumes/static:/modularhistory/_volumes/static
      - ./_volumes/redirects:/modularhistory/_volumes/redirects

  django:
    image: "ghcr.io/modularhistory/django:$SHA"
    command: bash config/scripts/init/django.sh
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      elasticsearch:
        condition: service_healthy
    deploy:
      restart_policy:
        condition: on-failure
    env_file: .env
    environment:
      <<: *common-env-vars
      DJANGO_SETTINGS_MODULE: "core.settings"
    healthcheck:
      test:
        [
          "CMD-SHELL",
          "curl --fail http://localhost:8000/healthcheck/ || exit 1"
        ]
      timeout: 7s
      interval: 30s
      retries: 3
      start_period: 60s
    expose:
      - "8000"
    user: www-data
    volumes:
      # NOTE: www-data must be granted permission to write to these directories
      # both in the container and on the host machine. Permissions to write in
      # the container are granted in Dockerfile.django. Permissions to write on
      # the host machine must be granted manually; e.g.,
      #     sudo chown -R www-data:www-data .backups && sudo chmod g+w -R .backups
      - ./_volumes/db/backups:/modularhistory/_volumes/db/backups
      - ./_volumes/static:/modularhistory/_volumes/static
      - ./_volumes/media:/modularhistory/_volumes/media
      - ./_volumes/redirects:/modularhistory/_volumes/redirects
      - es_certs:$ELASTIC_CERTS_DIR

  celery:
    image: "ghcr.io/modularhistory/django:$SHA"
    command: bash config/scripts/init/celery.sh
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      django:
        condition: service_healthy
    deploy:
      restart_policy:
        condition: on-failure
    env_file: .env
    environment:
      <<: *common-env-vars
      IS_CELERY: "True"
    healthcheck:
      test: celery -A core inspect ping -d celery@$$HOSTNAME
      timeout: 30s
      interval: 30s
      retries: 3
      start_period: 20s
    user: www-data
    volumes:
      - ./_volumes/db/backups:/modularhistory/_volumes/db/backups
      - ./_volumes/db/init:/modularhistory/_volumes/db/init
      - ./_volumes/static:/modularhistory/_volumes/static
      - ./_volumes/media:/modularhistory/_volumes/media
      - ./_volumes/redirects:/modularhistory/_volumes/redirects

  celery_beat:
    image: "ghcr.io/modularhistory/django:$SHA"
    command: bash config/scripts/init/celery_beat.sh
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      celery:
        condition: service_healthy
    deploy:
      restart_policy:
        condition: on-failure
    env_file: .env
    environment:
      <<: *common-env-vars
    healthcheck:
      test: [ "CMD-SHELL", "stat -t /tmp/celerybeat.pid || exit 1" ]
      timeout: 20s
      interval: 30s
      retries: 3
      start_period: 10s

  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:7.14.1
    depends_on:
      create_es_certs:
        condition: service_completed_successfully
    deploy:
      restart_policy:
        condition: on-failure
    env_file: .env
    environment:
      # https://www.elastic.co/guide/en/elasticsearch/reference/current/important-settings.html
      - node.name=modularhistory-es
      - cluster.name=es-docker-cluster
      - discovery.type=single-node
      - bootstrap.memory_lock=true
      - "ES_JAVA_OPTS=-Xms1536m -Xmx1536m"
      # TODO: reenable xpack.ml after switching to newer-hardware server
      - xpack.ml.enabled=false
      - xpack.security.enabled=true
      - xpack.security.http.ssl.enabled=true
      - xpack.security.http.ssl.key=$ELASTIC_CERTS_DIR/modularhistory-es/modularhistory-es.key
      - xpack.security.http.ssl.certificate_authorities=$ELASTIC_CERTS_DIR/ca/ca.crt
      - xpack.security.http.ssl.certificate=$ELASTIC_CERTS_DIR/modularhistory-es/modularhistory-es.crt
      - xpack.security.transport.ssl.enabled=true
      - xpack.security.transport.ssl.verification_mode=certificate
      - xpack.security.transport.ssl.certificate_authorities=$ELASTIC_CERTS_DIR/ca/ca.crt
      - xpack.security.transport.ssl.certificate=$ELASTIC_CERTS_DIR/modularhistory-es/modularhistory-es.crt
      - xpack.security.transport.ssl.key=$ELASTIC_CERTS_DIR/modularhistory-es/modularhistory-es.key
      - xpack.security.http.ssl.client_authentication=optional
    expose:
      - "9200"
    healthcheck:
      test: curl --cacert ${ELASTIC_CERTS_DIR}/ca/ca.crt -s https://localhost:9200
        >/dev/null; if [[ $$? == 52 ]]; then echo 0; else echo 1; fi
      interval: 30s
      timeout: 10s
      retries: 5
    ulimits:
      memlock:
        soft: -1
        hard: -1
    volumes:
      - es_data:/usr/share/elasticsearch/data
      - es_certs:$ELASTIC_CERTS_DIR

  create_es_certs:
    image: docker.elastic.co/elasticsearch/elasticsearch:7.14.1
    environment:
      ELASTIC_PASSWORD: $ELASTIC_PASSWORD
    command: >
      bash -c '
        if [[ ! -f /certs/bundle.zip ]]; then
          bin/elasticsearch-certutil cert --silent --pem --in "${ELASTIC_CERTS_DIR}/instances.yml" -out /certs/bundle.zip;
          unzip /certs/bundle.zip -d /certs; 
        fi;
        chown -R 1000:0 /certs
      '
    user: "0"
    working_dir: /usr/share/elasticsearch
    volumes:
      - es_certs:/certs
      - ./config/elasticsearch:$ELASTIC_CERTS_DIR

  kibana:
    image: docker.elastic.co/kibana/kibana:7.8.1
    depends_on:
      - elasticsearch
    env_file: .env
    environment:
      ELASTICSEARCH_URL: https://elasticsearch:9200
      ELASTICSEARCH_HOSTS: https://elasticsearch:9200
      ELASTICSEARCH_USERNAME: elastic
      # ELASTICSEARCH_PASSWORD: $ELASTIC_PASSWORD
    expose:
      - "5601"

  redis:
    image: redis
    command: redis-server /usr/local/etc/redis/redis.conf
    deploy:
      restart_policy:
        condition: on-failure
    expose:
      - "6379"
    # TODO: https://stackoverflow.com/questions/47088261/restarting-an-unhealthy-docker-container-based-on-healthcheck
    healthcheck:
      test: [ "CMD", "redis-cli", "ping" ]
      interval: 20s
      timeout: 10s
      retries: 3
      start_period: 20s
    volumes:
      - "data:/data"
      - ./config/redis:/usr/local/etc/redis

  postgres:
    image: postgres:14
    # deploy:
    #   restart_policy:
    #     condition: 'always'
    env_file: .env
    expose:
      - "5432"
    healthcheck:
      test: [ "CMD-SHELL", "pg_isready -U postgres" ]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./_volumes/db/init:/docker-entrypoint-initdb.d

  # mongo:
  #   deploy:
  #     restart_policy:
  #       condition: on-failure
  #       max_attempts: 3
  #   env_file: .env
  #   healthcheck:
  #     test: echo 'db.runCommand("ping").ok' | mongo localhost:27017/test --quiet
  #     interval: 10s
  #     timeout: 10s
  #     retries: 3
  #     start_period: 20s
  #   image: mongo
  #   expose:
  #     - "27017"
  #   volumes:
  #     - data:/data

  redisinsight:
    image: redislabs/redisinsight:latest
    profiles: [ "debug" ]
    depends_on:
      - redis
    deploy:
      restart_policy:
        condition: on-failure
        max_attempts: 3
    env_file: .env
    environment:
      REDIS_HOSTS: "local:redis:6379"
    healthcheck:
      test:
        [
          "CMD-SHELL",
          "curl --fail http://localhost:8002/healthcheck/ || exit 1"
        ]
      timeout: 7s
      interval: 15s
      retries: 2
      start_period: 10s
    ports:
      - "8002:8002"
    volumes:
      - "redisinsight:/db"

  cypress:
    # https://github.com/cypress-io/cypress-docker-images
    image: "cypress/included:8.4.1"
    depends_on:
      webserver:
        condition: service_healthy
    environment:
      - CYPRESS_baseUrl=http://modularhistory.dev.net:8080/
    working_dir: /e2e
    volumes:
      - ./frontend/cypress:/e2e/cypress
      - ./frontend/cypress.json:/e2e/cypress.json

volumes:
  # `data` is used by both redis and mongodb
  data: null
  letsencrypt: null
  certbot: null
  postgres_data: null
  es_data: null
  es_certs: null
  redisinsight: null
