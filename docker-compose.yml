version: "3.8"

x-common-env-vars:
  &common-env-vars
  DOCKERIZED: "True"
  REDIS_HOST: "redis"
  POSTGRES_HOST: "postgres"

services:
  next:
    image: "ghcr.io/modularhistory/frontend:$SHA"
    command: npm run start
    depends_on:
      - django
    deploy:
      restart_policy:
        condition: on-failure
    env_file: .env
    environment:
      PORT: 3002
      NEXTAUTH_URL: "https://www.modularhistory.orega.org"
      NEXTAUTH_URL_INTERNAL: "http://next:3002"
    ports:
      - "${PORT}:${PORT}"
    volumes:
      - ./_volumes/static:/_volumes/static
      - ./_volumes/redirects:/_volumes/redirects

  django:
    image: "ghcr.io/modularhistory/backend:$SHA"
    command: bash /.config/scripts/init/django.sh
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      elasticsearch:
        condition: service_healthy
    deploy:
      restart_policy:
        condition: on-failure
    env_file: .env
    environment:
      <<: *common-env-vars
      DJANGO_SETTINGS_MODULE: "core.settings"
    healthcheck:
      test:
        [
          "CMD-SHELL",
          "curl --fail http://localhost:${DJANGO_PORT}/healthcheck/ || exit 1"
        ]
      timeout: 7s
      interval: 30s
      retries: 3
      start_period: 60s
    ports:
      - "${DJANGO_PORT}:${DJANGO_PORT}"
    user: www-data
    volumes:
      # NOTE: www-data must be granted permission to write to these directories
      # both in the container and on the host machine. Permissions to write in
      # the container are granted in Dockerfile.backend. Permissions to write on
      # the host machine must be granted manually; e.g.,
      #     sudo chown -R www-data:www-data .backups && sudo chmod g+w -R .backups
      - ./_volumes/db/backups:/_volumes/db/backups
      - ./_volumes/static:/_volumes/static
      - ./_volumes/media:/_volumes/media
      - ./_volumes/redirects:/_volumes/redirects
      - ./.config:/.config
      - es_certs:${ELASTIC_CERTS_DIR}

  celery:
    image: "ghcr.io/modularhistory/backend:${SHA}"
    command: bash /.config/scripts/init/celery.sh
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      django:
        condition: service_healthy
    deploy:
      restart_policy:
        condition: on-failure
    env_file: .env
    environment:
      <<: *common-env-vars
      IS_CELERY: "True"
    healthcheck:
      test: celery -A core inspect ping -d celery@$$HOSTNAME
      timeout: 30s
      interval: 30s
      retries: 3
      start_period: 20s
    user: www-data
    volumes:
      - ./.config:/.config
      - ./_volumes/db/backups:/_volumes/db/backups
      - ./_volumes/db/init:/_volumes/db/init
      - ./_volumes/static:/_volumes/static
      - ./_volumes/media:/_volumes/media
      - ./_volumes/redirects:/_volumes/redirects

  celery_beat:
    image: "ghcr.io/modularhistory/backend:${SHA}"
    command: bash /.config/scripts/init/celery_beat.sh
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      celery:
        condition: service_healthy
    deploy:
      restart_policy:
        condition: on-failure
    env_file: .env
    environment:
      <<: *common-env-vars
    healthcheck:
      test: [ "CMD-SHELL", "stat -t /tmp/celerybeat.pid || exit 1" ]
      timeout: 20s
      interval: 30s
      retries: 3
      start_period: 10s
    volumes:
      - ./.config:/.config

  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:7.14.1
    depends_on:
      create_es_certs:
        condition: service_completed_successfully
    deploy:
      restart_policy:
        condition: on-failure
    env_file: .env
    environment:
      # https://www.elastic.co/guide/en/elasticsearch/reference/current/important-settings.html
      - node.name=modularhistory-es
      - cluster.name=es-docker-cluster
      - discovery.type=single-node
      - bootstrap.memory_lock=true
      - "ES_JAVA_OPTS=-Xms1536m -Xmx1536m"
      # TODO: reenable xpack.ml after switching to newer-hardware server
      - xpack.ml.enabled=false
      - xpack.security.enabled=true
      - xpack.security.http.ssl.enabled=true
      - xpack.security.http.ssl.key=${ELASTIC_CERTS_DIR}/modularhistory-es/modularhistory-es.key
      - xpack.security.http.ssl.certificate_authorities=${ELASTIC_CERTS_DIR}/ca/ca.crt
      - xpack.security.http.ssl.certificate=${ELASTIC_CERTS_DIR}/modularhistory-es/modularhistory-es.crt
      - xpack.security.transport.ssl.enabled=true
      - xpack.security.transport.ssl.verification_mode=certificate
      - xpack.security.transport.ssl.certificate_authorities=${ELASTIC_CERTS_DIR}/ca/ca.crt
      - xpack.security.transport.ssl.certificate=${ELASTIC_CERTS_DIR}/modularhistory-es/modularhistory-es.crt
      - xpack.security.transport.ssl.key=${ELASTIC_CERTS_DIR}/modularhistory-es/modularhistory-es.key
      - xpack.security.http.ssl.client_authentication=optional
    expose:
      - "9200"
    healthcheck:
      test: curl --cacert ${ELASTIC_CERTS_DIR}/ca/ca.crt -s https://localhost:9200 >/dev/null; if [[ $$? == 52 ]]; then echo 0; else echo 1; fi
      interval: 30s
      timeout: 10s
      retries: 5
    ulimits:
      memlock:
        soft: -1
        hard: -1
    volumes:
      - es_data:/usr/share/elasticsearch/data
      - es_certs:${ELASTIC_CERTS_DIR}

  create_es_certs:
    image: docker.elastic.co/elasticsearch/elasticsearch:7.14.1
    environment:
      ELASTIC_PASSWORD: $ELASTIC_PASSWORD
    command: >
      bash -c '
        if [[ ! -f /certs/bundle.zip ]]; then
          bin/elasticsearch-certutil cert --silent --pem --in "${ELASTIC_CERTS_DIR}/instances.yml" -out /certs/bundle.zip;
          unzip /certs/bundle.zip -d /certs; 
        fi;
        chown -R 1000:0 /certs
      '
    user: "0"
    working_dir: /usr/share/elasticsearch
    volumes:
      - es_certs:/certs
      - ./.config/elasticsearch:${ELASTIC_CERTS_DIR}

  kibana:
    image: docker.elastic.co/kibana/kibana:7.8.1
    depends_on:
      - elasticsearch
    env_file: .env
    environment:
      ELASTICSEARCH_URL: http://elasticsearch:9200
      ELASTICSEARCH_HOSTS: http://elasticsearch:9200
      ELASTICSEARCH_USERNAME: elastic
      # ELASTICSEARCH_PASSWORD: $ELASTIC_PASSWORD
    expose:
      - "5601"

  redis:
    image: redis
    command: redis-server /usr/local/etc/redis/redis.conf
    deploy:
      restart_policy:
        condition: on-failure
    expose:
      - "6379"
    # TODO: https://stackoverflow.com/questions/47088261/restarting-an-unhealthy-docker-container-based-on-healthcheck
    healthcheck:
      test: [ "CMD", "redis-cli", "ping" ]
      interval: 20s
      timeout: 10s
      retries: 3
      start_period: 20s
    volumes:
      - "data:/data"
      - ./.config/redis:/usr/local/etc/redis

  postgres:
    image: postgres:14
    # deploy:
    #   restart_policy:
    #     condition: 'always'
    env_file: .env
    expose:
      - "5432"
    healthcheck:
      test: [ "CMD-SHELL", "pg_isready -U postgres" ]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./_volumes/db/init:/docker-entrypoint-initdb.d

  # mongo:
  #   deploy:
  #     restart_policy:
  #       condition: on-failure
  #       max_attempts: 3
  #   env_file: .env
  #   healthcheck:
  #     test: echo 'db.runCommand("ping").ok' | mongo localhost:27017/test --quiet
  #     interval: 10s
  #     timeout: 10s
  #     retries: 3
  #     start_period: 20s
  #   image: mongo
  #   expose:
  #     - "27017"
  #   volumes:
  #     - data:/data

  redisinsight:
    image: redislabs/redisinsight:latest
    profiles: [ "debug" ]
    depends_on:
      - redis
    deploy:
      restart_policy:
        condition: on-failure
        max_attempts: 3
    env_file: .env
    environment:
      REDIS_HOSTS: "local:redis:6379"
    healthcheck:
      test:
        [
          "CMD-SHELL",
          "curl --fail http://localhost:8002/healthcheck/ || exit 1"
        ]
      timeout: 7s
      interval: 15s
      retries: 2
      start_period: 10s
    ports:
      - "${DJANGO_PORT}:${DJANGO_PORT}"
    volumes:
      - "redisinsight:/db"

  cypress:
    # https://github.com/cypress-io/cypress-docker-images
    image: "cypress/included:8.4.1"
    # depends_on:
    #   webserver:
    #     condition: service_healthy
    environment:
      - CYPRESS_baseUrl=http://modularhistory.dev.net:8080/
    working_dir: /e2e
    volumes:
      - ./frontend/cypress:/e2e/cypress
      - ./frontend/cypress.json:/e2e/cypress.json

volumes:
  # `data` is used by both redis and mongodb
  data: null
  letsencrypt: null
  certbot: null
  postgres_data: null
  es_data: null
  es_certs: null
  redisinsight: null
