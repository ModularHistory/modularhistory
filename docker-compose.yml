
version: "3.8"

x-common-env-vars: &common-env-vars
  DOCKERIZED: "True"
  REDIS_HOST: "redis"
  POSTGRES_HOST: "postgres"

services:
  webserver:
    image: "ghcr.io/modularhistory/webserver:$SHA"
    depends_on:
      - next
    deploy:
      restart_policy:
        condition: on-failure
    env_file: .env
    environment:
      DOMAINS: "modularhistory.com,www.modularhistory.com"
    ports:
      - "80:8080"
      - "443:8443"
    volumes:
      - ./_volumes/media:/modularhistory/_volumes/media
      - ./_volumes/static:/modularhistory/_volumes/static
      - ./_volumes/redirects:/modularhistory/_volumes/redirects
      # Keep the nginx config in a volume to allow for zero-downtime deploys via
      # `docker-compose exec webserver /usr/sbin/nginx -s reload`. Without this, the
      # container would always have to be stopped (so its static IP address becomes 
      # available) before starting a replacement container with an updated nginx config.
      - ./config/nginx/prod:/etc/nginx/conf.d
      - ./config/nginx:/modularhistory/config/nginx

  next:
    image: "ghcr.io/modularhistory/next:$SHA"
    command: npm run start
    depends_on:
      - django
      - celery
      - celery_beat
    deploy:
      restart_policy:
        condition: on-failure
    env_file: .env
    environment:
      NEXTAUTH_URL: "https://www.modularhistory.com"
      NEXTAUTH_URL_INTERNAL: "http://next:3000"
    expose:
      - "3000"
    volumes:
      - ./_volumes/static:/modularhistory/_volumes/static

  django:
    image: "ghcr.io/modularhistory/django:$SHA"
    command: bash config/scripts/init/django.sh
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      elasticsearch:
        condition: service_healthy
    deploy:
      restart_policy:
        condition: on-failure
    env_file: .env
    environment:
      <<: *common-env-vars
      DJANGO_SETTINGS_MODULE: "core.settings"
    healthcheck:
      test:
        [
          "CMD-SHELL",
          "curl --fail http://localhost:8000/healthcheck/ || exit 1",
        ]
      timeout: 7s
      interval: 30s
      retries: 3
      start_period: 60s
    expose:
      - "8000"
    user: www-data
    volumes:
      # NOTE: www-data must be granted permission to write to these directories
      # both in the container and on the host machine. Permissions to write in
      # the container are granted in Dockerfile.django. Permissions to write on
      # the host machine must be granted manually; e.g.,
      #     sudo chown -R www-data:www-data .backups && sudo chmod g+w -R .backups
      - ./_volumes/db/backups:/modularhistory/_volumes/db/backups
      - ./_volumes/static:/modularhistory/_volumes/static
      - ./_volumes/media:/modularhistory/_volumes/media
      - ./_volumes/redirects:/modularhistory/_volumes/redirects

  celery:
    image: "ghcr.io/modularhistory/django:$SHA"
    command: bash config/scripts/init/celery.sh
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      django:
        condition: service_healthy
    deploy:
      restart_policy:
        condition: on-failure
    env_file: .env
    environment:
      <<: *common-env-vars
      IS_CELERY: "True"
    healthcheck:
      test: celery -A core inspect ping -d celery@$$HOSTNAME
      timeout: 30s
      interval: 30s
      retries: 3
      start_period: 20s
    user: www-data
    volumes:
      - ./_volumes/db/backups:/modularhistory/_volumes/db/backups
      - ./_volumes/db/init:/modularhistory/_volumes/db/init
      - ./_volumes/static:/modularhistory/_volumes/static
      - ./_volumes/media:/modularhistory/_volumes/media
      - ./_volumes/redirects:/modularhistory/_volumes/redirects

  celery_beat:
    image: "ghcr.io/modularhistory/django:$SHA"
    command: bash config/scripts/init/celery_beat.sh
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      celery:
        condition: service_healthy
    deploy:
      restart_policy:
        condition: on-failure
    env_file: .env
    environment:
      <<: *common-env-vars

  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:7.14.1
    depends_on:
      create_es_certs:
        condition: service_completed_successfully
    deploy:
      restart_policy:
        condition: on-failure
    env_file: .env
    environment:
      # https://www.elastic.co/guide/en/elasticsearch/reference/current/important-settings.html
      - node.name=modularhistory-es
      - cluster.name=es-docker-cluster
      - discovery.type=single-node
      - bootstrap.memory_lock=true
      - "ES_JAVA_OPTS=-Xms1536m -Xmx1536m"
      # TODO: reenable xpack.ml after switching to newer-hardware server
      - xpack.ml.enabled=false
      - xpack.security.enabled=true
      # TODO
      - xpack.security.transport.ssl.enabled=true
      - xpack.security.transport.ssl.verification_mode=certificate
      - xpack.security.transport.ssl.keystore.path=${ELASTIC_CERTS_DIR}/elastic-certificates.p12
      - xpack.security.transport.ssl.truststore.path=${ELASTIC_CERTS_DIR}/elastic-certificates.p12
      - xpack.security.http.ssl.enabled=true
      - xpack.security.http.ssl.keystore.path=${ELASTIC_CERTS_DIR}/elastic-certificates.p12
      - xpack.security.http.ssl.truststore.path=${ELASTIC_CERTS_DIR}/elastic-certificates.p12
      - xpack.security.http.ssl.client_authentication=optional
    expose:
      - "9200"
    healthcheck:
      test: curl --cacert ${ELASTIC_CERTS_DIR}/client-ca.cer -s https://localhost:9200 >/dev/null; if [[ $$? == 52 ]]; then echo 0; else echo 1; fi
      interval: 30s
      timeout: 10s
      retries: 5
    ulimits:
      memlock:
        soft: -1
        hard: -1
    volumes:
      - es_data:/usr/share/elasticsearch/data
      - es_certs:$ELASTIC_CERTS_DIR

  create_es_certs:
    image: docker.elastic.co/elasticsearch/elasticsearch:7.14.1
    command: >
      bash -c '
        if [ ! -f "${ELASTIC_CERTS_DIR}/elastic-certificates.p12" ]; then
          bin/elasticsearch-certutil cert -out "${ELASTIC_CERTS_DIR}/elastic-certificates.p12" -pass ""
          openssl pkcs12 -in "${ELASTIC_CERTS_DIR}/elastic-certificates.p12" -nocerts -nodes > "${ELASTIC_CERTS_DIR}/client.key"
          openssl pkcs12 -in "${ELASTIC_CERTS_DIR}/elastic-certificates.p12" -clcerts -nokeys > "${ELASTIC_CERTS_DIR}/client.cer"
          openssl pkcs12 -in "${ELASTIC_CERTS_DIR}/elastic-certificates.p12" -cacerts -nokeys -chain > "${ELASTIC_CERTS_DIR}/client-ca.cer"
        fi;
        chown -R 1000:0 "$ELASTIC_CERTS_DIR"
      '
    user: "0"
    working_dir: /usr/share/elasticsearch
    volumes: ['es_certs:$ELASTIC_CERTS_DIR']

  kibana:
    image: docker.elastic.co/kibana/kibana:7.8.1
    depends_on:
      - elasticsearch
    env_file: .env
    environment:
      ELASTICSEARCH_URL: http://elasticsearch:9200
      ELASTICSEARCH_HOSTS: http://elasticsearch:9200
      ELASTICSEARCH_USERNAME: elastic
      # ELASTICSEARCH_PASSWORD: $ELASTIC_PASSWORD
    expose:
      - "5601"

  # mongo:
  #   deploy:
  #     restart_policy:
  #       condition: on-failure
  #       max_attempts: 3
  #   env_file: .env
  #   healthcheck:
  #     test: echo 'db.runCommand("ping").ok' | mongo localhost:27017/test --quiet
  #     interval: 10s
  #     timeout: 10s
  #     retries: 3
  #     start_period: 20s
  #   image: mongo
  #   expose:
  #     - "27017"
  #   volumes:
  #     - data:/data

  postgres:
    image: postgres
    deploy:
      restart_policy:
        condition: any
    env_file: .env
    expose:
      - "5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./_volumes/db/init:/docker-entrypoint-initdb.d

  redis:
    image: redis
    deploy:
      restart_policy:
        condition: on-failure
    expose:
      - "6379"
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 20s
      timeout: 10s
      retries: 3
      start_period: 20s
    volumes:
      - "data:/data"

  redisinsight:
    image: redislabs/redisinsight:latest
    profiles: ["debug"]
    depends_on:
      - redis
    deploy:
      restart_policy:
        condition: on-failure
        max_attempts: 3
    env_file: .env
    environment:
      REDIS_HOSTS: "local:redis:6379"
    healthcheck:
      test:
        [
          "CMD-SHELL",
          "curl --fail http://localhost:8002/healthcheck/ || exit 1",
        ]
      timeout: 7s
      interval: 15s
      retries: 2
      start_period: 10s
    ports:
      - "8002:8002"
    volumes:
      - "redisinsight:/db"
  
  cypress:
    # https://github.com/cypress-io/cypress-docker-images
    image: "cypress/included:7.1.0"
    depends_on:
      webserver:
        condition: service_healthy
    environment:
      - CYPRESS_baseUrl=https://www.modularhistory.com/
    working_dir: /e2e
    volumes:
      - ./frontend/cypress:/e2e/cypress
      - ./frontend/cypress.json:/e2e/cypress.json

volumes:
  # `data` is used by both redis and mongodb
  data:
  letsencrypt:
  certbot:
  postgres_data:
  es_data:
  es_certs:
  redisinsight:
